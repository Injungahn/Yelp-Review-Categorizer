{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45f06a-c006-4c54-9a6b-a7e6f248f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9770691-876a-42f3-9d1f-557e41825673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/San_Francisco_restaurant_reviews_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9cec8-75de-4021-b4c3-c4031f6f069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69690c-ca84-4567-8a7c-50a6c5d7fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b0309-49b3-4d50-83c0-e1ee410ffe56",
   "metadata": {},
   "source": [
    "## Checkign and getting rid of useless sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936a2a8-ea4f-4f66-bac6-46e340fa9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'].value_counts().iloc[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e15d66-2acd-4486-ac8f-3cf605992320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing sentences that are just a number and a period\n",
    "df['sentence'] = df['sentence'].apply(lambda x: re.sub(r'^\\d.$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53510a62-adc9-4621-b2ae-2079f913acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing sentences that aren't alphanumeric characters\n",
    "df['sentence'] = df['sentence'].apply(lambda x: re.sub(r'^\\W+$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed876e-5306-4106-af9c-ad089a43a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing sentences that are just a word and a colon\n",
    "df['sentence'] = df['sentence'].apply(lambda x: re.sub(r'^\\w+:$', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca91786-4396-4047-b31a-321fcf975385",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df[df['sentence'] != '']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6f99e-1ef5-4888-a64a-d1b56a901c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentence'].value_counts().iloc[0:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bb96a-7770-4629-ac15-3005236d20da",
   "metadata": {},
   "source": [
    "## Lemmatizing sentences and getting the count of occurences of each of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3d6b5-5259-4b9c-b606-39ab6ad52d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lemmatizer and tokenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378aa1d-5dc3-4ed1-b49b-90e9c14a1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing and tokenizing sentences\n",
    "df['lemmatized'] = df['sentence'].apply(lambda x: [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(x.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6f482-f716-4a98-a923-630e8dd48eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0bd17-cea4-4318-a6fc-30b364a6b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords from lemmatized tokens\n",
    "df['lemmatized'] = df['lemmatized'].apply(lambda x: [token for token in x if token not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1830f3b-7fba-453f-b3b0-acd48e3c2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word_df by extrending all of the lemmatized lists\n",
    "\n",
    "word_df = []\n",
    "\n",
    "# Going through each row and appending the words to the word df\n",
    "for idx, row in df.iterrows():\n",
    "    word_df.extend(row['lemmatized'])\n",
    "                   \n",
    "word_df = pd.DataFrame(word_df, columns=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b22b94-380c-4376-90e4-faa0840016d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing df to be a count of each word found\n",
    "word_df = word_df['word'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4ca7d-f204-4e0e-8ffa-a65e9e05221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11110da0-a7a9-4607-bfc4-715294a1bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9201af-5a0e-446d-b888-216c6a0a5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_greater_than_1000 = word_df[word_df >= 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5a6ba-e42e-4d37-b31d-11d458ac0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_greater_than_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510fc5d-72bb-4083-8ddb-3b48d144127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barchart_of_words(counts, words, title, ):\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    y_pos = np.arrange(10)\n",
    "    plt.rc('axes', titlesize=30) \n",
    "    plt.rc('axes', labelsize=20) \n",
    "    plt.rc('xtick', labelsize=15)\n",
    "    plt.rc('ytick', labelsize=15) \n",
    "    plt.barh(y_pos ,counts)\n",
    "    plt.yticks(y_pos, words)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../images/top_{title[5:7]}_most_frequent_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb26d5-b76e-4c9e-a4b7-c8abfc2ff181",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value in zip(word_count_greater_than_1000[600:].index, word_count_greater_than_1000[600:]):\n",
    "    print(idx, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e07880-2708-4673-84f6-34e32fe70d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_words = ['food', 'get', 'delicious', 'dish' ,'ordered', 'order', 'pizza', 'flavor', 'menu',\n",
    "             'sauce', 'chicken', 'meat', 'fresh', 'pork', 'bit', 'burrito', 'bread', 'taste',\n",
    "             'fried', 'eat', 'salad', 'dinner', 'cheese', 'drink', 'oyster', 'sweet', 'ordered', 'try', \n",
    "             'fish', 'egg', 'brunch', 'potato', 'sandwich', 'rice', 'spicy', 'seafood', 'shrimp',\n",
    "             'clam', 'tried', 'coffee', 'dessert', 'crab', 'portion', 'toast', 'tasty', 'bacon', 'taco',\n",
    "             'soup', 'beef', 'plate', 'big', 'course', 'crispy', 'chip', 'dumpling', 'cooked', 'tasted',\n",
    "             'lunch', 'quality', 'flavorful', 'cream', 'huge', 'excellent', 'perfectly', 'noodle',\n",
    "             'eating', 'wine', 'appetizer', 'chowder', 'slice', 'served', 'roll', 'rib', 'large', 'piece',\n",
    "             'pancake', 'tender', 'breakfast', 'style', 'fry' , 'bean', 'benedict', 'cut', 'cocktail', \n",
    "             'steak', 'ramen', 'soft', 'chocolate', 'light', 'lemon', 'crust', 'half', 'yummy', 'sausage',\n",
    "             'entree', 'texture', 'sushi', 'filling', 'bowl', 'grilled', 'wing', 'lamb', 'korean', 'butter',\n",
    "             'tea', 'ordering', 'start', 'ingredient', 'mushroom', 'tomato', 'onion', 'salmon', 'salsa',\n",
    "             'curry', 'water', 'broth', 'cioppino',' veggie', 'mexican', 'started', 'ate', 'cold' , 'duck',\n",
    "             'creamy', 'belly', 'mouth', 'juicy', 'salty', 'burger', 'ice', 'thick', 'rich', 'thai', 'scallop'\n",
    "             'chef', 'savory', 'vegan', 'avocado', 'shared', 'pasta', 'spice', 'red', 'crunchy', \n",
    "             'chop', 'pepper', 'yum', 'vegetarian', 'seasoned', 'thin', 'roasted', 'kimchi', 'beignet',\n",
    "             'combo', 'tart', 'heavy', 'risotto', 'stuffed', 'italian', 'bakery', 'generous', 'asada',\n",
    "             'mussel', 'lobster', 'sour', 'serving', 'pastry', 'ricotta', 'topping' , 'spinach', 'fusion',\n",
    "             'bone', 'authentic', 'vegetable', 'craving', 'carne', 'banana', 'coconut', 'croissant', 'apple',\n",
    "             'fluffy']\n",
    "\n",
    "service_words = ['service', 'staff', 'friendly', 'waiter', 'served', 'check' , 'customer', 'finally', \n",
    "                 'ask', 'attentive', 'told', 'tip', 'waitress']\n",
    "time_words = ['wait', 'line', 'reservation', 'minute', 'hour', 'night', 'seated', 'server', 'waiting',\n",
    "              'busy', 'early', 'waited', 'quick', 'quickly', 'fast', 'weekend', 'saturday', 'morning',\n",
    "              'sunday', 'waitlist', 'evening', 'serve', 'month', 'friday', 'crowded']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194b2fb-4e22-4ce2-914f-c650cbaf9c42",
   "metadata": {},
   "source": [
    "Created a list of words to filter through the messages by looking at words that were used at least 1000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21b219-cd36-4493-a54b-404a540566cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_set = set(food_words)\n",
    "service_set = set(service_words)\n",
    "time_set = set(time_words)\n",
    "\n",
    "df['food_review'] = False\n",
    "df['service_review'] = False\n",
    "df['time_review'] = False\n",
    "\n",
    "df['food_review'] = df['lemmatized'].apply(lambda x: set(x)&food_set)\n",
    "df['service_review'] = df['lemmatized'].apply(lambda x: set(x)&service_set)\n",
    "df['time_review'] = df['lemmatized'].apply(lambda x: set(x)&time_set)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64fbc6-9db7-4632-b7aa-214829f7c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['food_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96a3c3-0c19-4e4c-bdb5-09f67b4de9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['food_review'] != set()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e16d85-71ef-4d5c-916f-60a048f7a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['service_review'] != set()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a2869-cc3a-480e-9b7f-9a855f2714c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['time_review'] != set()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04844b6-d95e-4aca-9c99-2665fc8f1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_specific_reviews = df[(df['food_review'] == set()) & (df['service_review'] == set()) & (df['time_review'] == set())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66eac11-2114-4b56-b4c6-86b31f7da790",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in non_specific_reviews['sentence'].iloc[12:12].iteritems():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cd43e-969b-4345-b817-344310703dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
